# DEEP LEARNING

Neden KullanÄ±rÄ±z ?

- karmaÅŸÄ±k problemleri Ã§Ã¶zebilmek
- yapÄ±sal olmayan verileri analiz edebilmek
- geliÅŸtirilen modelin performansÄ±nÄ± arttÄ±rabilmek
- kurumsal dÃ¶nÃ¼ÅŸÃ¼m ve teknolojiye ayak uydurabilmek

NEDÄ°R ?

![](resim1.png)

YAPAY ZEKA: Ä°nsan zekasÄ±nÄ±n bilgisayarlar, yazÄ±lÄ±mlar veya makineler aracÄ±lÄ±ÄŸÄ±yla taklit edilmesi sÃ¼recidir.

MAKÄ°NE Ã–ÄRENMESÄ°: BilgisayarlarÄ±n geÃ§miÅŸ verilerden Ã¶ÄŸrenerek tahmin veya karar verebilmesini saÄŸlayan yÃ¶ntemler bÃ¼tÃ¼nÃ¼dÃ¼r.

DERÄ°N Ã–ÄRENME: Yapay sinir aÄŸ yapÄ±sÄ±nÄ± kullanrak bÃ¼yÃ¼k ve karmaÅŸÄ±k veri kÃ¼melerinden Ã¶ÄŸrenme yapabilen makine Ã¶ÄŸrenmesi yÃ¶ntemi

### GEN AI - LLM

Derin Ã¶ÄŸrenme modellerini kullanarak yeni veri Ã¼reten modellere (LLM) GenAI denir.

# Machine Learning vs Deep Learning

| Ã–zellik                       | Machine Learning                                    | Deep Learning                                      |
| ----------------------------- | --------------------------------------------------- | -------------------------------------------------- |
| **YÃ¶ntemler**                 | Geleneksel algoritma ve istatistiksel yÃ¶ntemler     | Yapay sinir aÄŸlarÄ± ve Ã§ok katmanlÄ± yapÄ±lar         |
| **Hesaplama Maliyeti**        | Daha dÃ¼ÅŸÃ¼k                                          | YÃ¼ksek                                             |
| **Veri TÃ¼rÃ¼**                 | Tabular veriler                                     | GÃ¶rÃ¼ntÃ¼, ses, video                                |
| **BÃ¼yÃ¼k Veri ile GeliÅŸtirme** | Zor                                                 | Daha esnek                                         |
| **Ã–zellik MÃ¼hendisliÄŸi**      | BaÅŸarÄ± oranÄ±nda etkisi yÃ¼ksek, Ã¶zellik Ã§Ä±karÄ±mÄ± zor | Ã–zellik Ã§Ä±karÄ±mÄ± daha standart, verimliliÄŸi yÃ¼ksek |
| **AÃ§Ä±klanabilirlik**          | Daha yÃ¼ksek                                         | Daha dÃ¼ÅŸÃ¼k                                         |

### Deep Learning Terminoloji

Bir yapay sinir aÄŸÄ±,

- birbiri ile baÄŸlantÄ±lÄ± dÃ¼ÄŸÃ¼mler (nodes)
- nÃ¶ronlar
- katman yapÄ±larÄ±ndan oluÅŸur.

Derin Ã–ÄŸrenme; Yapay sinir aÄŸlarÄ±nÄ±n fazla sayÄ±da katmana sahip olmasÄ± durumudur.

![](resim2.png)

### Deep Learning AlgoritmalarÄ±nda KullanÄ±lan KÃ¼tÃ¼phaneler

- TensorFlow
- PyTorch
- Keras
- mxnet

### Derin Ã–ÄŸrenme UygulamalarÄ±

- gÃ¶rÃ¼ntÃ¼ tanÄ±ma, gÃ¶rÃ¼ntÃ¼ iÅŸleme
- doÄŸal dil iÅŸleme (NLP)
- ses tanÄ±ma
- otonom araÃ§lar

### Deep Learning nasÄ±l Ã§alÄ±ÅŸÄ±yor?

- Ã–nce girdi verilerini (mesela bir resim) alÄ±yoruz ve iÃ§inden Ã§Ä±ktÄ± tahmini Ã¼retiyoruz.

- Sonra bu tahmini, elimizdeki gerÃ§ek cevapla karÅŸÄ±laÅŸtÄ±rÄ±yoruz.

- Aradaki farkÄ± Ã¶lÃ§mek iÃ§in loss function denen bir deÄŸer hesaplÄ±yoruz. (Bu aslÄ±nda â€œne kadar hata yaptÄ±kâ€ demek.)

- Bu hata yÃ¼ksekse, aÄŸÄ±rlÄ±klarÄ± ve bias deÄŸerlerini kÃ¼Ã§Ã¼k kÃ¼Ã§Ã¼k deÄŸiÅŸtirerek hatayÄ± azaltmaya Ã§alÄ±ÅŸÄ±yoruz. (Buna optimizasyon deniyor.)

- AmaÃ§: hata (loss) deÄŸerini mÃ¼mkÃ¼n olduÄŸunca kÃ¼Ã§Ã¼ltmek.

-EÄŸer model sadece eÄŸitim verisini ezberlerse (train Ã§ok iyi ama test kÃ¶tÃ¼), bu durumda model aÅŸÄ±rÄ± Ã¶ÄŸrenmiÅŸ olur â†’ overfitting.

- O yÃ¼zden Ã¶ÄŸrenmeyi fazla uzatmamalÄ±, tam kararÄ±nda bÄ±rakmalÄ±yÄ±z.

## ![](resim3.png)

## **NOT: BÄ°AS deÄŸeri** sayesinde Ã§Ä±ktÄ±larÄ± daha baÅŸarÄ±lÄ± ÅŸekilde gÃ¼ncelleyip daha iyi sonuÃ§lar elde edebiliriz. Ã–rneÄŸin sadece weight (w) deÄŸerlerini gÃ¼ncelleyerek ilerleseydik tek bir noktadan gÃ¼ncelleme yaptÄ±ÄŸÄ±mÄ±z iÃ§in modelin baÅŸarÄ± performansÄ±nÄ± istediÄŸimiz gibi yÃ¼kseltemeyebilirdik. Bu nedenle bias ekliyoruz.

Modelin neden genellenebilir olmasÄ±nÄ± istiyoruz ?

- EÄŸer sÃ¼rekli katsayÄ±larÄ± (w)geliÅŸtiriyorsak ama loss artÄ±k dÃ¼ÅŸmÃ¼yorsa, model zaten benzer hatalarÄ± Ã¶ÄŸrenmiÅŸ ve onlarÄ± dÃ¼zeltecek ÅŸekilde ayarlanmÄ±ÅŸtÄ±r. Bu noktadan sonra gÃ¼ncellemeler aynÄ± veya Ã§ok benzer sonuÃ§larÄ± verir.

---

Aktivasyon Fonksiyonunun Ã–nemi

- Aktivasyon fonksiyonlarÄ±, yapay sinir aÄŸlarÄ±nda gizli katmanlarda aÄŸÄ±rlÄ±k (ğ‘¤) ve bias (ğ‘) ile hesaplanan deÄŸerden sonra uygulanÄ±r ve modele **karmaÅŸÄ±k ÅŸeyleri Ã¶ÄŸrenme yeteneÄŸi** kazandÄ±rÄ±r; giriÅŸ katmanÄ±nda aktivasyon kullanÄ±lmaz Ã§Ã¼nkÃ¼ orasÄ± sadece veriyi iÃ§eri alÄ±r, Ã§Ä±kÄ±ÅŸ katmanÄ±nda ise **probleme gÃ¶re Ã¶zel bir aktivasyon** seÃ§ilir (Ã¶rneÄŸin ikili sÄ±nÄ±flandÄ±rmada Sigmoid, Ã§ok sÄ±nÄ±fta Softmax, sayÄ±sal tahminde aktivasyon yoktur).

Neden probleme Ã¶zel aktivasyon seÃ§ilir ?

- Aktivasyon fonksiyonunu probleme gÃ¶re seÃ§mek Ã¶nemlidir Ã§Ã¼nkÃ¼ her problem farklÄ± bir Ã§Ä±ktÄ± ister: mesela ikili sÄ±nÄ±flandÄ±rmada Sigmoid kullanÄ±lÄ±r ve sonucu 0â€“1 arasÄ±nda olasÄ±lÄ±k gibi verir, Ã§ok sÄ±nÄ±flÄ± problemlerde Softmax seÃ§ilir ve her sÄ±nÄ±fa olasÄ±lÄ±k daÄŸÄ±tÄ±r, sayÄ±sal tahminlerde ise genelde aktivasyon kullanÄ±lmaz ki model her tÃ¼rlÃ¼ gerÃ§ek sayÄ±yÄ± tahmin edebilsin; yani doÄŸru aktivasyon seÃ§mek, modelin Ã§Ä±ktÄ±sÄ±nÄ± probleme uygun ve anlamlÄ± hale getirir.

---

**SOFTMAX Aktivasyonu Ã–rneÄŸi:**

```
import tensorflow as tf
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Dense
import numpy as np

# 1ï¸âƒ£ SÄ±nÄ±f isimlerini tanÄ±mlÄ±yoruz
class_names = ["Kedi", "KÃ¶pek", "KuÅŸ"]

# 2ï¸âƒ£ Modelimizi oluÅŸturuyoruz
model = Sequential([
    Dense(8, activation='relu', input_shape=(4,)),   # gizli katman: 8 nÃ¶ron, giriÅŸte 4 Ã¶zellik var
    Dense(3, activation='softmax')                   # Ã§Ä±kÄ±ÅŸ katmanÄ±: 3 sÄ±nÄ±f iÃ§in softmax
])

# 3ï¸âƒ£ Modeli derliyoruz (optimizer, loss, metric)
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# 4ï¸âƒ£ Ã–rnek giriÅŸ verisi (4 Ã¶zellikli bir veri)
# Diyelim ki elimizde "bir hayvan" var, 4 sayÄ±yla temsil edilmiÅŸ
sample_data = np.array([[0.5, 0.8, 0.2, 0.9]])  # [Ã¶zellik1, Ã¶zellik2, Ã¶zellik3, Ã¶zellik4]

# 5ï¸âƒ£ Model tahmini (rastgele aÄŸÄ±rlÄ±klarla eÄŸitimsiz Ã¶rnek)
prediction = model.predict(sample_data)

# 6ï¸âƒ£ Sonucu yazdÄ±rÄ±yoruz
print("OlasÄ±lÄ±klar:", prediction)  # her sÄ±nÄ±f iÃ§in olasÄ±lÄ±k
predicted_class = np.argmax(prediction)  # en yÃ¼ksek olasÄ±lÄ±ÄŸÄ±n indexi
print("Tahmin edilen sÄ±nÄ±f:", class_names[predicted_class])
```

---

#### Sigmoid Fonksiyonu

FormÃ¼l:
Sigmoid(x) = 1 / (1 + e^(-x))

- Ã‡Ä±ktÄ± daima **0 ile 1** arasÄ±ndadÄ±r.

---

### Aktivasyon Fonksiyonu TÃ¼rleri

1. Linear : Regresyon modelleri
2. Sigmoid : Ä°kili sÄ±nÄ±flandÄ±rma
3. TANH (Hyperbolic Tangent): Zaman serisi modelleri (Ã–R: SatÄ±ÅŸlarÄ± zaman periyoduna gÃ¶re Ã¶lÃ§me) -gizli katmanlarda
4. ReLU (Rectified Linear Unit): GÃ¶rÃ¼ntÃ¼ iÅŸleme -gizli katmanlarda
5. Softmax : Ã§oklu sÄ±nÄ±flandÄ±rma

---

#### Cost Fonksiyonu

Regresyon Problemleri iÃ§in kullanÄ±lÄ±r

Modelin tahminlerinin gerÃ§ek deÄŸerden ne kadar farklÄ± olduÄŸunu Ã¶lÃ§er.- performans deÄŸerlendirmesi

- MSE: (GerÃ§ek deÄŸer âˆ’ Tahmin edilen deÄŸer)Â² alÄ±nÄ±r, tÃ¼m Ã¶rneklerin ortalamasÄ± hesaplanÄ±r.

---

### Log-Loss Fonksiyonu

SÄ±nÄ±flandÄ±rma problemleri iÃ§in kullanÄ±lÄ±r.

---

#### Forward Propagation (Ä°leri YayÄ±lÄ±m) adÄ±mlarÄ±:

- girdiyi al
- aÄŸÄ±rlÄ±klarÄ± Ã§arp ve bias ekle
- aktivasyon fonksiyonu uygula
- sonraki katmana ilet
- Ã§Ä±kÄ±ÅŸ hesaplanana kadar tekrarla

---

#### Backpropagation adÄ±mlarÄ±:

- loss fonksiyonu hesaplamak
- zincir kuralÄ± ile gradyanlarÄ± hesapla
- aÄŸÄ±rlÄ±klarÄ± gÃ¼ncelle
- epoch boyunca tekrar et

Modelin her katmandaki aÄŸÄ±rlÄ±klarÄ±n ve biaslarÄ±n kaybÄ± minimize edecek ÅŸekilde nasÄ±l ayarlanmasÄ± gerektiÄŸini belirler.

- paramatrelerin gÃ¼ncellenmesi _update kuralÄ±_ ile belirlenir.

---

## CNN (Convolutional Neural Network) MÄ°MARÄ°SÄ°

![](resim4.jpg)

Convolution Layers

- Ä°lgili resmin Ã¶zelliklerini algÄ±lamaktan sorumlu.
- Bu katmanda, gÃ¶rÃ¼ntÃ¼deki dÃ¼ÅŸÃ¼k ve yÃ¼ksek seviyeli Ã¶zellikleri Ã§Ä±karmak iÃ§in resme bazÄ± filtreler ya da kerneller uygularÄ±z
- CNN'de gÃ¶rÃ¼ntÃ¼ler matrislerle ifade edilir.

Pooling Layers

- Ã‡Ä±karÄ±lan Ã¶zelliklerin (feature map) boyutunu kÃ¼Ã§Ã¼ltÃ¼r.
- AmaÃ§:giriÅŸ matrisinin kanal sayÄ±sÄ±nÄ± sabit tutarak geniÅŸlik ve yÃ¼kseklik bazÄ±nda boyut indirgemesi yapmak. Bu sayede hesaplama karmaÅŸÄ±klÄ±ÄŸÄ±nÄ± azaltÄ±r. **EvriÅŸim katmanÄ±ndan sonra** kullanÄ±lÄ±r.
  - Max Pooling
  - Avarage Pooling

Flatten & Dense Layers

- Flatten layer, Ã§ok boyutlu veriyi (Ã¶rneÄŸin 28x28 piksellik bir resim) tek boyutlu vektÃ¶re dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r ki tam baÄŸlÄ± (Dense) katmanlara girdi olarak verilebilsin.

- Dense layer ise her nÃ¶ronun Ã¶nceki katmandaki tÃ¼m nÃ¶ronlarla baÄŸlantÄ±lÄ± olduÄŸu, aÄŸÄ±rlÄ±k (ğ‘¤) ve bias (ğ‘) kullanarak hesap yapÄ±p aktivasyon fonksiyonu ile Ã§Ä±ktÄ± Ã¼reten katmandÄ±r.

```
from tensorflow.keras import layers, Sequential

model = Sequential([
    # 1ï¸âƒ£ GiriÅŸ katmanÄ±: 224x224 boyutunda, 3 kanallÄ± (RGB) resimler
    layers.Input(shape=(224, 224, 3)),

    # 2ï¸âƒ£ Ä°lk Convolutional katman: 32 filtre, 3x3 kernel, ReLU aktivasyon
    layers.Conv2D(32, (3, 3), activation='relu'),
    # 3ï¸âƒ£ MaxPooling: 2x2 havuzlama ile boyutu kÃ¼Ã§Ã¼ltme
    layers.MaxPooling2D((2, 2)),

    # 4ï¸âƒ£ Ä°kinci Convolutional katman: 64 filtre, 3x3 kernel
    layers.Conv2D(64, (3, 3), activation='relu'),
    layers.MaxPooling2D((2, 2)),

    # 5ï¸âƒ£ ÃœÃ§Ã¼ncÃ¼ Convolutional katman: 128 filtre, 5x5 kernel
    layers.Conv2D(128, (5, 5), activation='relu'),

    # 6ï¸âƒ£ Ã‡ok boyutlu veriyi dÃ¼zleÅŸtirme (Flatten)
    layers.Flatten(),

    # 7ï¸âƒ£ Tam baÄŸlÄ± (Dense) katman: 128 nÃ¶ron, ReLU aktivasyonu
    layers.Dense(128, activation='relu'),

    # 8ï¸âƒ£ Ã‡Ä±kÄ±ÅŸ katmanÄ±: 3 sÄ±nÄ±f iÃ§in 3 nÃ¶ron, Softmax aktivasyonu
    layers.Dense(3, activation='softmax')
])
```

_Ã–zet:_ Bu model bir resmi alÄ±yor â†’ Conv ve Pooling katmanlarÄ±yla Ã¶zelliklerini Ã§Ä±karÄ±yor â†’ Flatten ile dÃ¼zleÅŸtiriyor â†’ Dense katmanlarda iÅŸliyor â†’ en sonunda Softmax ile â€œbu resim hangi sÄ±nÄ±fa aitâ€ kararÄ±nÄ± veriyor.

---

| Avantajlar                | Dezavantajlar               |
| ------------------------- | --------------------------- |
| Ã–zellik Ã§Ä±karÄ±mÄ±          | Hesaplama maliyeti          |
| Spatial invariance        | Overfitting                 |
| GÃ¼rÃ¼ltÃ¼ye karÅŸÄ± dayanÄ±klÄ± | Yorumlanabilirlik eksikliÄŸi |
| Transfer Learning         |                             |
| Performans                |                             |

---

### Pretrained CNN Model

Pretrained CNN model, bÃ¼yÃ¼k veri kÃ¼meleri (Ã¶r. ImageNet) Ã¼zerinde Ã¶nceden eÄŸitilmiÅŸ, temel gÃ¶rsel Ã¶zellikleri (kenar, ÅŸekil, doku) zaten Ã¶ÄŸrenmiÅŸ ve bu bilgileri transfer learning ile yeni gÃ¶revlerde kullanÄ±labilen derin sinir aÄŸÄ± modelidir.

- kaynak ve zaman maliyeti
- performans
- Ã¶zelleÅŸtirme ve adaptasyon

---

#### Pretrained CNN Model - VGG

VGG, her yerde aynÄ± 3x3 filtrelerden oluÅŸan standart konvolÃ¼syon bloklarÄ±yla homojen bir yapÄ± kurar ve eÄŸitimde aÄŸÄ±rlÄ±klarÄ± **Xavier yÃ¶ntemiyle** baÅŸlatarak derin aÄŸlarda Ã¶ÄŸrenmenin dengeli ve kararlÄ± ilerlemesini saÄŸlar.

---

#### Pretrained CNN Model - ResNet

ResNet, pretrained CNN modelleri arasÄ±nda Ã¶ne Ã§Ä±kan; â€œresidual connectionâ€ (skip connection) denilen atlama baÄŸlantÄ±larÄ± sayesinde Ã§ok derin katmanlarda bile kaybolan gradyan sorununu Ã§Ã¶zerek daha derin, daha doÄŸru ve kararlÄ± Ã¶ÄŸrenme saÄŸlayan, bÃ¼yÃ¼k veri kÃ¼melerinde Ã¶nceden eÄŸitilip transfer learning iÃ§in yaygÄ±n kullanÄ±lan gÃ¼Ã§lÃ¼ bir modeldir.

---

#### Pretrained CNN Model - GoogleNet(Inception)

GoogleNet (Inception), pretrained CNN modellerinden biri olup, her katmanda farklÄ± boyutlarda filtreleri (1x1, 3x3, 5x5) aynÄ± anda kullanarak hem geniÅŸ hem derin Ã¶zellik Ã§Ä±karÄ±mÄ± yapabilen, parametre sayÄ±sÄ±nÄ± azaltmak iÃ§in Inception bloklarÄ±yla optimize edilmiÅŸ ve ImageNet Ã¼zerinde eÄŸitilip transfer learningâ€™de sÄ±kÃ§a kullanÄ±lan bir mimaridir.

-BÃ¼yÃ¼k katmanlÄ± yapÄ±lardan iyi seviye Ã¶zellik Ã§Ä±kartmak iÃ§in tercih edilir.

---

### TRANSFER LEARNING - FINE TUNING

**Transfer learning,** Ã¶nceden eÄŸitilmiÅŸ bir modeli yeni bir gÃ¶reve uyarlamayÄ± saÄŸlar; **fine-tuning** ise bu modeli Ã¶zelleÅŸtirerek en iyi sonucu vermesi iÃ§in ayarlama yapar.

---

### Fine Tuning Teknikleri

- Feature Extractor (Ã–zellik Ã‡Ä±karmak) : Modelin Ã¶nceden Ã¶ÄŸrendiÄŸi Ã¶zellikler (kenar, ÅŸekil, desen vb.) kullanÄ±lÄ±r, sadece son katman yeniden eÄŸitilir.
- Full Network Fine Tuning: HiÃ§bir katman dondurulamaz. TÃ¼m model, yeni veri setine gÃ¶re baÅŸtan sona yeniden eÄŸitilerek Ã¶zelleÅŸtirilir.
- KatmanlarÄ±n dondurulmasÄ± (Frozen layers): Modelin bazÄ± katmanlarÄ± sabitlenir (aÄŸÄ±rlÄ±klarÄ± deÄŸiÅŸtirilmez). Sadece seÃ§ilen katmanlar yeniden eÄŸitilir.
- Gradual Unfreezing: Katmanlar yavaÅŸ yavaÅŸ â€œdondurulmuÅŸâ€ durumdan Ã§Ä±karÄ±lÄ±r. Ã–nce Ã¼st katmanlar, sonra alt katmanlar aÃ§Ä±larak model daha verimli ÅŸekilde adapte edilir.

Ã–ZET TABLO:

### Fine Tuning Teknikleri

| Teknik                       | AÃ§Ä±klama                                                                                                |
| ---------------------------- | ------------------------------------------------------------------------------------------------------- |
| **Feature Extractor**        | Ã–nceden Ã¶ÄŸrenilmiÅŸ Ã¶zellikler kullanÄ±lÄ±r, sadece son katman yeniden eÄŸitilir.                           |
| **Full Network Fine Tuning** | HiÃ§bir katman dondurulmaz, tÃ¼m model yeni veri setine gÃ¶re baÅŸtan sona yeniden eÄŸitilir.                |
| **Frozen Layers**            | BazÄ± katmanlar sabitlenir (aÄŸÄ±rlÄ±klarÄ± deÄŸiÅŸmez), sadece belirli katmanlar yeniden eÄŸitilir.            |
| **Gradual Unfreezing**       | Katmanlar aÅŸamalÄ± olarak dondurulmuÅŸ durumdan Ã§Ä±karÄ±lÄ±r, Ã¶nce Ã¼st katmanlar sonra alt katmanlar aÃ§Ä±lÄ±r. |

---
